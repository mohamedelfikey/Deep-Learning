{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a01a112",
   "metadata": {},
   "source": [
    "# LR Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015cadd",
   "metadata": {},
   "source": [
    "## 1. Predetermined Piecewise Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9622d9b",
   "metadata": {},
   "source": [
    "#### In predetermined piecewise scheduling, the learning rate is manually defined for specific epochs. It remains constant for a defined number of steps/epochs and then reduces.\n",
    "\n",
    "##### Advantages:\n",
    "###### - Simple and easy to implement.\n",
    "###### - Useful when you have domain knowledge or experience with similar tasks.\n",
    "\n",
    "##### Disadvantages:\n",
    "###### - Requires manual tuning and experimentation.\n",
    "###### - Not adaptive to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "boundaries = [10, 20, 30]  # At these epochs, learning rate will change\n",
    "values = [0.1, 0.01, 0.001, 0.0001]  # Learning rate at each stage\n",
    "\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries, values)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f824c80",
   "metadata": {},
   "source": [
    "## 2. Performance-Based Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbfa88",
   "metadata": {},
   "source": [
    "#### In performance-based scheduling (also called ReduceLROnPlateau), the learning rate is reduced when the model's performance (e.g., validation loss) stops improving.\n",
    "\n",
    "###### Advantages:\n",
    "###### - Adaptive and reduces the learning rate when progress slows.\n",
    "###### - Helps in fine-tuning the model in later stages of training.\n",
    "\n",
    "##### Disadvantages:\n",
    "###### - May result in overfitting if the learning rate is reduced too quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels, validation_data=(val_data, val_labels),\n",
    "          epochs=50, callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18be902",
   "metadata": {},
   "source": [
    "## 3. Exponential Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2628f",
   "metadata": {},
   "source": [
    "#### Exponential decay reduces the learning rate by multiplying it by a factor (usually less than 1) at each step or epoch.\n",
    "\n",
    "##### Advantages:\n",
    "###### - Smooth decay in learning rate helps fine-tuning.\n",
    "###### - Can accelerate convergence initially.\n",
    "\n",
    "##### Disadvantages:\n",
    "###### - May reduce learning rate too quickly if not configured properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=10000, decay_rate=0.96, staircase=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34226cc1",
   "metadata": {},
   "source": [
    "## 4. Power Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b70b4f",
   "metadata": {},
   "source": [
    "#### Power scheduling reduces the learning rate at a polynomial rate with respect to time. The learning rate is typically reduced at a rate proportional to ùë° ** ‚àíùëùùëúùë§ùëíùëü , where t is the epoch number.\n",
    "\n",
    "##### Advantage:\n",
    "###### - Provides a more controlled decay compared to exponential scheduling.\n",
    "###### - Useful in large-scale problems where fine-tuning is crucial.\n",
    "\n",
    "##### Disadvantages:\n",
    "###### - Needs careful tuning of the power parameter.\n",
    "##### - Reduces the learning rate slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate, decay_steps=10000, end_learning_rate=0.0001, power=0.5)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
